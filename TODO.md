# √âtape 1 : Pr√©parer, nettoyer et enrichir les donn√©es
Vous allez d√©couvrir et pr√©parer les donn√©es n√©cessaires √† la construction de votre mod√®le de scoring.
Cela inclut le nettoyage, la fusion des diff√©rentes sources, l‚Äôencodage des variables et la cr√©ation de nouvelles features pertinentes.
L‚Äôobjectif est de constituer un dataset propre et enrichi, pr√™t pour l‚Äôentra√Ænement. Vous devrez √©galement analyser la qualit√© de vos variables et les d√©s√©quilibres dans les classes.

## Pr√©requis
- Avoir explor√© les donn√©es brutes fournies.
- Avoir v√©rifi√© les formats et les valeurs manquantes.
- Avoir identifi√© les colonnes cl√©s pour les jointures.
- Avoir pris en compte les enjeux m√©tiers (par exemple : d√©s√©quilibre des classes).

## R√©sultat attendu
Un jeu de donn√©es propre, fusionn√© et enrichi, pr√™t √† √™tre utilis√© pour l‚Äôentra√Ænement.

## Recommandations
- Charger chaque fichier s√©par√©ment et inspecter ses colonnes.
- Utiliser pandas pour fusionner les jeux de donn√©es.
- Visualiser la distribution des classes cibles.
- Cr√©er de nouvelles features √† partir des variables existantes si n√©cessaire.
- √âviter de supprimer trop rapidement les colonnes avec des valeurs manquantes : explorer les possibilit√©s d‚Äôimputation.

## Points de vigilance
- Oublier de v√©rifier les doublons.
- Supprimer des colonnes sans analyser leur importance m√©tier.
- Imputer sans documenter ni justifier.
- Fusionner sans g√©rer les duplications ou pertes de lignes.
- Encoder sans tenir compte du type de mod√®le pr√©vu (ordinal vs nominal).

## Outils
- pandas
- matplotlib et seaborn pour la visualisation
- scikit-learn pour le preprocessing
- missingno pour visualiser les valeurs manquantes

# √âtape 2 : Traquer les exp√©rimentations avec MLflow
Vous allez tracer vos exp√©riences de mod√©lisation avec MLflow : m√©triques, hyperparam√®tres, versions de mod√®les, etc.
Vous utiliserez l‚Äôinterface web pour visualiser vos runs et comparer les mod√®les.

## Pr√©requis
- Avoir install√© MLflow.
- Avoir configur√© votre projet localement.

## R√©sultat attendu
Des runs visibles dans l‚ÄôUI MLflow avec les param√®tres test√©s et les scores obtenus.

## Recommandations
- Commencer par int√©grer mlflow.start_run() dans vos notebooks.
- Logger les m√©triques et les param√®tres principaux.
- Utiliser mlflow.autolog() si vous utilisez des mod√®les compatibles.
- Activer l‚Äôinterface UI avec mlflow ui pour visualiser les r√©sultats.

## Points de vigilance
- Lancer MLflow sans environnement isol√© peut cr√©er des conflits de versions. Utiliser un environnement virtuel.
- Oublier d‚Äôannoter les exp√©riences (tags, noms, commentaires) complique l‚Äôanalyse dans l‚Äôinterface MLflow.
- Ne pas versionner les mod√®les enregistr√©s emp√™che de reproduire les r√©sultats et de g√©rer leur cycle de vie.
- Sauvegarder des fichiers inutiles ou trop volumineux dans MLflow encombre le syst√®me et ralentit l‚Äôinterface.

## Outils
- MLflow

# √âtape 3 : Mod√©liser et exp√©rimenter avec plusieurs algorithmes
Vous allez entra√Æner diff√©rents mod√®les de classification et comparer leurs performances sur des m√©triques m√©tiers et classiques.
L‚Äôobjectif est de tester plusieurs familles de mod√®les (for√™ts, boosting, MLP, etc.) et de construire une premi√®re version de votre pipeline d‚Äôapprentissage.
Vous devez aussi int√©grer une validation crois√©e pour √©valuer leur robustesse.

## Pr√©requis
- Avoir pr√©par√© un dataset propre et pr√™t √† l‚Äôentra√Ænement.
- Avoir compris la nature d√©s√©quilibr√©e du jeu de donn√©es.
- Avoir identifi√© les variables cibles et explicatives.
- Avoir install√© les biblioth√®ques de machine learning n√©cessaires.
- Avoir param√©tr√© MLflow.

## R√©sultat attendu
Un ou plusieurs mod√®les entra√Æn√©s, avec validation crois√©e et premi√®res m√©triques d‚Äô√©valuation.

## Recommandations
- Commencer par tester des mod√®les simples (Logistic Regression, Random Forest).
- Comparer ensuite avec des mod√®les plus puissants (XGBoost, LightGBM, MLP).
- Utiliser StratifiedKFold pour conserver la distribution des classes et garantir une √©valuation robuste.
- Entra√Æner les mod√®les dans des notebooks clairs et document√©s.
- Stocker les scores et les hyperparam√®tres test√©s.

## Points de vigilance
- Ne pas tester sans validation crois√©e. Une √©valuation bas√©e uniquement sur un split train/test unique peut produire des r√©sultats tr√®s variables selon le hasard du d√©coupage, et conduire √† des conclusions erron√©es sur la performance r√©elle d‚Äôun mod√®le.
- Ne pas comparer les mod√®les avec des m√©triques inadapt√©es. Privil√©gier des m√©triques pertinentes, telles que :
- AUC-ROC,
- Recall sur la classe minoritaire,
- F1-score,
- Co√ªt m√©tier personnalis√© (ùêπ ùëÅ ‚â´ ùêπùëÉ).
- Ne pas oublier la stratification. Sans stratification, certains algorithmes peuvent se biaisent vers la classe majoritaire si le dataset contient beaucoup plus de bons que de mauvais clients.
- Ne pas ignorer le d√©s√©quilibre des classes. Utiliser un class_weight adapt√© ou du sur-√©chantillonnage (SMOTE, etc.) pour √©viter de biaiser l‚Äôapprentissage.

## Outils
- scikit-learn
- XGBoost
- LightGBM

# √âtape 4 : Optimiser les hyperparam√®tres et le seuil m√©tier
Vous allez optimiser les hyperparam√®tres des mod√®les pour maximiser leurs performances selon des crit√®res m√©tier. Vous d√©finirez √©galement un seuil de d√©cision optimal bas√© sur le co√ªt des erreurs.
L‚Äôobjectif est de minimiser le co√ªt m√©tier total (avec un poids plus fort sur les faux n√©gatifs que sur les faux positifs).

## Pr√©requis
- Avoir entra√Æn√© plusieurs mod√®les.
- Avoir compar√© leurs performances de base.
- Avoir compris la notion de co√ªt d‚Äôerreur.
- Avoir d√©fini une fonction de co√ªt m√©tier.

## R√©sultat attendu
Un mod√®le avec hyperparam√®tres optimis√©s et un seuil m√©tier ajust√©.

## Recommandations
- Utiliser GridSearchCV ou Optuna pour l‚Äôoptimisation.
- D√©finir une fonction de co√ªt pond√©rant les erreurs FN et FP.
- Tester diff√©rents seuils de classification (par exemple de 0.1 √† 0.9).
- Tracer la courbe co√ªt vs. seuil pour identifier la meilleure d√©cision.

## Points de vigilance
- Garder le seuil par d√©faut (0.5) sans justification. Ce seuil ne refl√®te pas n√©cessairement les enjeux m√©tiers : il doit √™tre optimis√© selon le ratio FN/FP.
- Oublier de tracer le score m√©tier en fonction du seuil : cela emp√™che d‚Äôidentifier la meilleure d√©cision.
- Optimiser uniquement sur l‚ÄôAUC ou l‚Äôaccuracy : ces m√©triques ne refl√®tent pas toujours les pertes m√©tier.
- Oublier d‚Äôadapter les m√©triques aux besoins business.
- Choisir un mod√®le sans tester sa robustesse.

## Outils
- scikit-learn (GridSearchCV)
- Optuna

## R√©sultat attendu :
un mod√®le final optimis√© et justifi√©.